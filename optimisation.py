# We aim to adjust weights and biases to reduce the loss function
# randomly tweak the weights and biases to see if the loss function decreases
# this is called gradient descent
# gradient descent is an iterative process and we use calculus to find the minimum of the loss function
# we can use the derivative of the loss function to find the minimum
# the derivative of the loss function is called the gradient
# we can use the gradient to adjust the weights and biases
# we can use the gradient to find the direction of the steepest descent

# def f(x):
#     return 2*x + 1
# derivative of f(x) = 2
# gradient = 2

# how do we measure with non-linear functions?
# we can use the chain rule to find the gradient of the loss function
# we can use the chain rule to find the gradient of the loss function with respect to the weights and biases

